{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import sklearn.metrics.pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Distance and Similarity\n",
    "\n",
    "# 5.1 Distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The distance metric qantifies the distance between two points $x$ and $y$. \n",
    "\n",
    "Some frequently used distance metrics include:\n",
    "- Euclidean distance: $$D_{euc}(x, y) = \\left(\\sum_{i=1}^{n}(x_{i}-y_{i})^2\\right)^{0.5}$$\n",
    "- Manhattan distance: $$D_{manh}(x, y) = \\sum_{i=1}^{n} abs(x_{i}-y_{i})$$\n",
    "\n",
    "Fun note: If you have run into the concepts elsewhere, Euclidian distance of a vector with itself is equivalent to the L2 norm of the vector, and the Manhattan distance corresponds to the L1 norm.\n",
    "\n",
    "In general, distance metrics satisfy some properties such as:\n",
    "- The distance between two points is non-negative. \n",
    "- The distance from a point to the point itself is equal to 0. \n",
    "- The distance from point $i$ to point $j$ is exactly equal to the distance from point $j$ to point $i$ (symmetry). \n",
    "- The distance from point $i$ to point $j$ via point $k$ is always longer than the direct distance from $i$ to $j$ (triangle inequality). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan distance:  2\n"
     ]
    }
   ],
   "source": [
    "# function to calculate manhattan distance between two points x and y\n",
    "def manhattan_distance(x, y):\n",
    "    diff = x - y\n",
    "    absolute_diff = numpy.absolute(diff)\n",
    "    dist = numpy.sum(absolute_diff)\n",
    "    return dist\n",
    "        \n",
    "x = numpy.array([1,3])\n",
    "y = numpy.array([2,4])\n",
    "print(\"Manhattan distance: \", manhattan_distance(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.1.1\n",
    "\n",
    "Create a function to calculate the Euclidean distance between two points x and y. \n",
    "Make sure that you do not use loops!\n",
    "\n",
    "For example:\n",
    "```python\n",
    "a = numpy.array([0.0, 0.0])\n",
    "b = numpy.array([1.0, 1.0])\n",
    "print(euclidean(a, b))\n",
    "```\n",
    "Output:\n",
    "```\n",
    "1.41421356237\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.1.2\n",
    "\n",
    "Load the iris data into a 150x4 numpy array. Compute the Euclidean distance between the first row and each other row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Distance for categorical data\n",
    "\n",
    "The most common method of computing the the distance across categorical dimensions is Jaccard's coefficient. This measures the ratio between the number of shared features between two objects (the intersection) and the toal number of features across the two objects (the union). Thus objects that share more features are more similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Distance for String data\n",
    "\n",
    "Distance measures for comparing strings with each other:\n",
    "- Hamming distance: number of positions at which two strings of the same length differ.\n",
    "    $$D_{hamming}(x, y) =  \\sum_{i=1}^{n} x_{i} \\neq y_{i} $$\n",
    "- Levenshtein distance: the minimum number of operations required to transform one string into another string. The operations are insertion, deletion and substitutions and each operations has a cost (often unit costs are used). The Levenshtein distance can be used in a lot of text mining techniques where the similarity between two words needs to be measured. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.1.3\n",
    "Create your own function to calculate the Hamming distance between two strings. The strings need to have the same length.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "a = 'Panama'\n",
    "b = 'Pamela'\n",
    "hamming(a, b)\n",
    "```\n",
    "Output:\n",
    "```\n",
    "3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Similarity\n",
    "\n",
    "A similarity metric quantifies how similar two objects are. Often, similarity is coded as a non-linear transformation of the inverse of the distance between two objects. This is intiutive from the definitions of the concepts: as the distance between two objects increases (all other things being equal) then the similarity should decrease, and vice versa.\n",
    "\n",
    "A very common similarity metric in data science is the cosine similarity. This measure treats $x$ (a point in a $n$-dimensional space) as a vector of length $n$. The cosine similarity between $x$ and $y$ is the dot product between $x$ and $y$ divided by the product of the L2-norms of $x$ and $y$. By normalizing by the lenth of the vectors (dividing by the L2-norms), the cosine similarity reflects the angle between the two vectors in high dimensional space. When the cosine similarity metric is equal to 1 the vectors point in exactly the same direction, when the similarity metric is equal to -1 then the vectors are the opposite of each other, and perpendicular vectors have a cosine similarity of 0.\n",
    "\n",
    "The formal definition of cosine similarity is: $$cosine(x, y) = \\frac{x^Ty}{||x||\\cdot ||y||} = \\frac{\\sum_{i=1}^n x_i y_i}{\\left(\\sum_{i=1}^n x_{i}^2\\right)^{0.5} \\left(\\sum_{i=1}^n y_{i}^2\\right)^{0.5}}$$\n",
    "\n",
    "In text-mining the cosine similarity can be used to represent the similarity between two documents.\n",
    "Each document is represented by a vector that has a dimension for each word in the corpus. The value on each vector is equal to the word frequency.\n",
    "\n",
    "The cosine similarity between two documents thus represents how similar they are in the set of words they contain. Documents that contain a lot of the same words the documents are said to be more similar than documents with hardly any overlapping words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2.1\n",
    "\n",
    "Create a function `cosine_similarity` that calculates the cosine similarity between two vectors. Do not use for-loops, Scipy, or Scikit-learn.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "a = numpy.array([0.0, 1.0])\n",
    "b = numpy.array([1.0, 0.0])\n",
    "c = numpy.array([0.0, -1.0])\n",
    "print(cosine_similarity(a, b))\n",
    "print(cosine_similarity(a, a))\n",
    "print(cosine_similarity(a, c))\n",
    "```\n",
    "Output:\n",
    "```\n",
    "0.0\n",
    "1.0\n",
    "-1.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Using scikit-learn to compute distances and similarity\n",
    "\n",
    "In most cases, there is no need to create your own code/function to calculate the distances.\n",
    "In the library `scikit-learn` the class `DistanceMetric` will calculate most distances for you.\n",
    "\n",
    "Scikit-learn is a library for machine learning algorithms in Python. Scikit-learn includes functions for classification, regression, clustering, dimensionality reduction, model selection and preprocessing. \n",
    "For more information:  http://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Distance measures in in scipy and scikit-learn\n",
    "\n",
    "The class to calculate distances is in the module \"neighbors\" of the scikit-learn library.\n",
    "One can access the class via: `sklearn.neighbors.DistanceMetric`\n",
    "Note that you first have to import the library `sklearn` and more specifically you have to import `sklearn.neighbors`.\n",
    "\n",
    "The `DistanceMetric` class includes different distance metrics, such as Euclidean and Manhattan. Also other distance metrics are present, such as chebyshev, minkowski, mahalanobis, haversine, hamming, canberra, braycurtis and many more. \n",
    "\n",
    "For more information about the distance metrics that are included in the `DistanceMetric` class:  http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n",
    "\n",
    "The most important methods of the `DistanceMetric` class are the `get_metric()` and the `pairwise()` functions. \n",
    "The `get_metric` function is used to specify which metric is used, for example 'euclidean' or 'manhattan'. \n",
    "The `pairwise` function is used to calculate the pairwise distances between points in an array x. \n",
    "See the examples below.\n",
    "\n",
    "There is also a simple function to calculate pairwise distances according to a number of metrics:\n",
    "```python\n",
    "from sklearn.metrics.pairwise import pairwise_distances```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance:\n",
      "[[ 0.          1.41421356  3.        ]\n",
      " [ 1.41421356  0.          2.23606798]\n",
      " [ 3.          2.23606798  0.        ]]\n",
      "\n",
      "[[ 0.          1.41421356  3.        ]\n",
      " [ 1.41421356  0.          2.23606798]\n",
      " [ 3.          2.23606798  0.        ]]\n",
      "\n",
      "Manhattan distance:\n",
      "[[ 0.  2.  3.]\n",
      " [ 2.  0.  3.]\n",
      " [ 3.  3.  0.]]\n",
      "\n",
      "[[ 0.  2.  3.]\n",
      " [ 2.  0.  3.]\n",
      " [ 3.  3.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.neighbors\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# we want to calculate the pairwise distance between three points in a 2D space\n",
    "x = numpy.array([[1,3], [2, 4], [1, 6]])\n",
    "\n",
    "# Euclidean distance\n",
    "d1 = sklearn.neighbors.DistanceMetric.get_metric('euclidean')\n",
    "print(\"Euclidean distance:\")\n",
    "print(d1.pairwise(x))\n",
    "print()\n",
    "print(pairwise_distances(x, metric='euclidean'))\n",
    "print()\n",
    "\n",
    "# Manhattan distance\n",
    "d2 = sklearn.neighbors.DistanceMetric.get_metric('manhattan')\n",
    "print(\"Manhattan distance:\")\n",
    "print(d2.pairwise(x))\n",
    "print()\n",
    "print(pairwise_distances(x, metric='manhattan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Cosine similarity in scipy and scikit-learn\n",
    "\n",
    "The cosine similarity is implemented in the scipy library and in scikit-learn library.\n",
    "\n",
    "In scipy:\n",
    "`scipy.spatial.distance.cosine(x,y)` calculates the cosine distance between two points `x` and `y` where `x` and `y` are both represented by a 1-dimensional vector.\n",
    "Note that in order to convert the distance to similarity, you should subtract distance from 1.\n",
    "\n",
    "In scikit-learn:\n",
    "`sklearn.metrics.pairwise.cosine_similarity(x)` calculates the pairwise cosine similarities between all points in array `x`.\n",
    "\n",
    "\n",
    "**IMPORTANT** Verify, and remember, which of these implementations work with sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5\n"
     ]
    }
   ],
   "source": [
    "# cosine similarity in scipy \n",
    "import scipy.spatial.distance\n",
    "x = [1, 0, -1]\n",
    "y = [-1,-1, 0]\n",
    "print(1 - scipy.spatial.distance.cosine(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-0.5  1. ]]\n"
     ]
    }
   ],
   "source": [
    "# pairwise cosine similarity in sklearn\n",
    "import sklearn.metrics.pairwise\n",
    "x = numpy.array([[1, 0, -1], [-1,-1, 0]])\n",
    "print(sklearn.metrics.pairwise.cosine_similarity(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3.1\n",
    "\n",
    "Use the function `word_count` which you implemented last week to create a document-term matrix from the first 1000 rows of file [coco_val.txt](coco_val.txt). \n",
    "- Does your cosine function work on rows of the sparse document-term matrix? If not, do you know why not?\n",
    "- Compute the pairwise cosine similarity between the rows of your document-term matrix using the `sklearn` implementation. Which document is the most similar to the first row according to cosine similarity?  \n",
    "- Based on the above experiment, suggest ways of modifying the word counts to make the cosine similarity more useful as a metric of text similarity. Implement your idea and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "text = []\n",
    "\n",
    "with open(\"coco_val.txt\") as myfile:\n",
    "    for index in range(N):\n",
    "        line = next(myfile)\n",
    "        text.append(line.split())\n",
    "        \n",
    "# vocab, M = word_count(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3.2\n",
    "\n",
    "Scikit learn has a couple of classes which are useful for creating various versions of document-word matrices:\n",
    "\n",
    "- `sklearn.feature_extraction.text.CountVectorizer`\n",
    "- `sklearn.feature_extraction.text.TfidfVectorizer`\n",
    "\n",
    "Read the documentation of these classes and try to apply them on the [coco_val.txt](coco_val.txt) data. Compute similarities/distances using these a few versions of these document-word matrices and check how they compare to using plain word-counts as we have been doing so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
